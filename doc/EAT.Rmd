---
title: "EAT: Efficiency Analysis Trees"
date: "`r Sys.Date()`"
author: "Miguel HernÃ¡ndez University"
output: 
  rmarkdown::html_vignette:
    # self_contained: no
vignette: >
  %\VignetteIndexEntry{EAT: Efficiency Analysis Trees}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
  body {
    text-align: justify;
    }
</style>


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  warning = FALSE,
  message = FALSE
)
```

This vignette is intended to known the main functions of the `eat` package. [Efficiency Analysis Trees](https://www.sciencedirect.com/science/article/pii/S0957417420306072) is an algorithm that estimates a production frontier in a data-driven environment by adapting regression trees. In this way, techniques from the field of **machine learning** are incorporated into solving problems in the field of **production theory**. From the latter, the following terminology is introduced.

Let us consider $n$ Decision Making Units (DMUs) to be evaluated. $DMU_i$ consumes $\textbf{x}_i = (x_{1i}, ...,x_{mi}) \in R^{m}_{+}$ amount of inputs for the production of $\textbf{y}_i = (y_{1i}, ...,y_{si}) \in R^{s}_{+}$ amount of outputs. The relative efficiency of each DMU in the sample is assessed with reference to the so-called production prossibility set or technology, which is the set of technically feasible combinations of $(\textbf{x, y})$. It is defined in general terms as:

\begin{equation}
    \Psi = \{(\textbf{x, y}) \in R^{m+s}_{+}: \textbf{x} \text{ can produce } \textbf{y}\}
\end{equation}

Monotonicity (free disposability) of inputs and outputs is assumed, meaning that if $(\textbf{x, y}) \in \Psi$, then $(\textbf{x', y'}) \in \Psi$, as soon as $\textbf{x'} \geq \textbf{x}$ and $\textbf{y'} \leq \textbf{y}$. Often convexity of $\Psi$ is also assumed. The efficient frontier of $\Psi$ may be defined as $\partial(\boldsymbol{\Psi}) := \{(\boldsymbol{x,y}) \in \boldsymbol{\Psi}: \boldsymbol{\hat{x}} < \boldsymbol{x}, \boldsymbol{\hat{y}} > \boldsymbol{y} \Rightarrow (\boldsymbol{\hat{x},\hat{y}}) \notin \boldsymbol{\Psi} \}$. Technical inefficiency is defined as the distance from a point that belongs to $\Psi$ to the production frontier $\partial(\Psi)$. For a point located inside $\Psi$, it is evident that there are many possible paths to the frontier, each associated with a different technical inefficiency measure.

In this section, an **EAT** model, an **RFEAT** model, a **FDH** model and a **DEA** model refer to a modeling carried out using Efficiency Analysis Trees technique, Random Forest + Efficiency Analysis Trees technique, Free Disposal Hull method and Data Envelopment Analysis method respectively. Additionally, a **CEAT** model refers to a convex **EAT** model. The functions developed in the `eat` library are always oriented to one of the four previous models (EAT, RFEAT, FDH or DEA) and can be divided into seven categories depending on its purpose:
  
```{r table, echo = FALSE}
library(dplyr)

functions <- data.frame("Purpose" = c(rep("Modeling", 2),
                                      rep("Summarise", 5),
                                      rep("Tuning", 2), 
                                      rep("Graph", 3),
                                      rep("Calculating efficiency scores", 3), 
                                      rep("Graph efficiency scores", 2),
                                      rep("Predict", 3), 
                                      rep("Rank", 2)), 
                        "Function name" = c("EAT", "RFEAT",
                                            "print", "summary", "size", "frontier.levels", "descrEAT",
                                            "bestEAT", "bestRFEAT", 
                                            "frontier", "plotEAT", "plotRFEAT",
                                            "efficiencyEAT", "efficiencyCEAT", "efficiencyRFEAT",
                                            "efficiencyDensity", "efficiencyJitter",
                                            "predictEAT", "predictRFEAT", "predictFDH",
                                            "rankingEAT", "rankingRFEAT"), 
                        "Usage" = c("Apply Efficiency Analysis Trees technique to a data frame. Return an EAT object.",
                                    "Apply Random Forest + Efficiency Analysis Trees technique to a data frame. Return a RFEAT object.",
                                    "For an EAT object: print the tree structure of an EAT model. 
                                    For a RFEAT object: print a brief summary of a RFEAT model.",
                                    "For an EAT object: return a summary of the leaf nodes, general information about the model and the error and
                                    threshold for each split and surrogate split.",
                                    "Return the number of leaf nodes for an EAT model.",
                                    "Return the frontier output levels for an EAT model.",
                                    "Return measures of centralization and dispersion with respect to the outputs for the leaf nodes 
                                    for an EAT model.",
                                    "Tune an EAT model.",
                                    "Tune a RFEAT model.",
                                    "Plot the estimated frontier through an EAT model in a low dimensional scenario
                                    (FDH estimated frontier is optional).",
                                    "Plot the tree structure of an EAT model.",
                                    "Shows a line graph with the OOB error on the y-axis calculated from a forest made up of k trees (x-axis).",
                                    "Calculate DMU efficiency scores through an EAT model (through a FDH model is optional).",
                                    "Calculate DMU efficiency scores through a convex EAT model (through a DEA model is optional).",
                                    "Calculate DMU efficiency scores through a RFEAT model (through a FDH model is optional).",
                                    "Graph a density plot for a data frame of efficiency scores (EAT, FDH, CEAT, DEA and RFEAT are available).",
                                    "Graph a jitter plot for a vector of efficiency scores calculated through an EAT model 
                                    (EAT or CEAT scores are accepted).",
                                    "Predict the output through an EAT model.",
                                    "Predict the output through a RFEAT model.",
                                    "Predict the output through a FDH model.",
                                    "Calculate variable importance scores through an EAT model.",
                                    "Calculate variable importance scores through a RFEAT model.")
)

kableExtra::kable(functions) %>%
  kableExtra::kable_styling("striped", full_width = F) %>%
  kableExtra::collapse_rows(columns = 1, valign = "middle") %>%
  kableExtra::row_spec(c(1:2, 8:9, 13:15, 18:20), background = "#DBFFD6") %>%
  kableExtra::row_spec(c(3:7, 10:12, 16:17, 21:22), background = "#FFFFD1") 
```

The `PISAindex` database is included as a data object in the `eat` library and is employed to exemplify the package functions. On the one hand, the inputs correspond to 13 variables that define the socioeconomic context of a country by means of a score in the range [1-100], except for the Gross Domestic Product by Purchasing Power Parity, which is measured in thousands of dollars. All of them have been obtained from the [Social Progress Index](https://www.socialprogress.org/). On the other hand, the performance of each country in the PISA exams is measured by the average score of its schools in the disciplines of Science, Reading and Mathematics and they have been collected from [PISA 2018 Results](https://www.oecd.org/pisa/Combined_Executive_Summaries_PISA_2018.pdf).

The following variables are collected for 72 countries that take the PISA exam:
  
* **Country**, **Continent** and a 3-letter code that identifies the country following the ISO 3166 ALPHA-3 as rownames.

* *Outputs* :
  
  * **S_PISA** : mean score on the PISA exam in Science.
  * **R_PISA** : mean score on the PISA exam in Reading.
  * **M_PISA** : mean score on the PISA exam in Mathematics.

* *Inputs* :
  
  * Basic Human Needs field : 
  
    * **NBMC** : Nutrition and Basic Medical Care.
    * **WS** : Water and Sanitation.
    * **S** : Shelter.
    * **PS** : Personal Safety.

  * Foundations of Wellbeing field :
  
    * **ABK** : Access to Basic Knowledge.
    * **AIC** : Access to Information and Communications. 
    * **HW** : Health and Wellness.
    * **EQ** : Environmental Quality.

  * Opportunity field :
  
    * **PR** : Personal Rights.
    * **PFC** : Personal Freedom and Choice.
    * **I** : Inclusiveness.
    * **AAE** : Access to Advanced Education.

  * **GDP_PPP** : Gross Domestic Product based on Purchasing Power Parity.

The `eat` package is applied with the following purposes: (1) to create homogeneous groups of countries in terms of their socioeconomic characteristics (Basic Human Needs, Foundations of Wellbeing, Opportunity and GDP PPP per capita) and subsequently to know for each of these groups, what is the maximum PISA score expected in a specific discipline or in more than one; (2) to know which countries exercise best practices and which of them do not obtain a performance according to their socioeconomic level; and (3) to know what variables are more relevant in obtaining efficient levels of output. 

```{r seed}
# We save the seed for reproducibility of the results
set.seed(120)
```

```{r library}
library(eat)
data("PISAindex")
```

## Modeling a scenario with an input and an output. Plotting the frontier

### EAT()

The `EAT` function is the centerpiece of the `eat` library. `EAT` performs a regression tree based on CART methodology under a new approach that guarantees obtaining a frontier as estimator that fulfills the property of free disposability. This new technique has been baptized as Efficiency Analysis Trees. The development of the functions contained in the `eat` library has been thought so that even true R novices can easily use the library. The minimum arguments of the function are the data (`data`) containing the study variables, the indexes of the predictor variables or inputs (`x`) and the indexes of the predicted variables or outputs (`y`). Additionally, the `numStop`, `fold` and `max.depth` arguments are included for those more experienced users in the fields of machine learning and tree-based models. Modifying these three allows obtaining different frontiers and therefore selecting the one that best suits the needs of the analysis. 

* `numStop` refers to the minimum number of observations in a node to be divided and is directly related to the size of the tree. The higher the value of `numStop` the smaller the size of the tree.

* `fold` refers to the number of parts in which the dataset is divided to apply the cross-validation technique. Variations in the `fold` argument are not directly related to the size of the tree.

* `max.depth` determines the maximum number of leaf nodes. When this argument is introduced, the typical process of growth-pruning is not carried out. In this case, the tree grows until the required number of leaf nodes is reached, and then, the tree is returned.

The error of a given node $t$ is measured as the prediction error at the node $t$ over the total number of observations:
  \begin{equation}
R(t) = \frac{n(t)}{N} \cdot MSE(t) = \frac{1}{N} \cdot \sum_{(x_i,y_i)\in t}(y_i - \hat{y}(t))^2
\end{equation}

The impurity of a tree $T$ is measured as the sum of the impurities for each leaf node.
\begin{equation}
R(T) = \sum_{i = 1}^{\widetilde{T}}R(t_i),
\end{equation}

where $\widetilde{T}$ is the set of leaf nodes for the tree $T$.

The function returns an `EAT` object.

```{r EAT, eval = FALSE}
EAT(data, x, y, 
    fold = 5,
    numStop = 5,
    max.depth = NULL,
    na.rm = TRUE)
```

* Example 1: `M_PISA ~ PFC`

```{r single.output, collapse = FALSE}
single_model <- EAT(data = PISAindex, 
                    x = 15, # input 
                    y = 3) # output
```

`print()` returns the tree structure for an `EAT` object where:
  
  * `y` : vector of predictions.
  * `R` : error at the node.
  * `n(t)` : number of DMUs at the node.
  * `input name < / >= s` represents the division of the space.
  * `<*>` indicates a leaf node.

```{r print.single.output, collapse = FALSE}
print(single_model)
```

`summary()` returns the following information of an `EAT` object:
  
  * `Formula` : outputs ~ inputs

  * `Summary for leaf nodes` where:
  
    * `id` : leaf node index.
    * `n(t)` : number of DMUs at the leaf node.
    * `%` : proportion of DMUs at the leaf node.
    *  As many columns as `output names` with the corresponding predictions for the leaf nodes.
    * `R(t)` : error at the leaf node.

  * `Tree` where:
  
    * `Inner nodes` : number of inner nodes.
    * `Leaf nodes` : number of leaf nodes.
    * `Total nodes` : total number of nodes (inner nodes + leaf nodes).
    * `R(T)` : error at the model.
    * `numStop` : numStop hyperparameter value.
    * `fold` : fold hyperparameter value.
    * `max.depth` : max.depth hyperparameter value.

  * `Primary & surrogate splits` where:
  
    * `Node A --> {B, C}` indicates that the node A is split into the left child node B and the right child node C.
    * `variable --> {R: , s: }` represents the division of the space with its error and threshold.
    * `Surrogate splits` indicates the best possible split for each variable that has not been used to divide the node. This is expressed as `variable --> {R: , s: }` where `R` is the error at the node and `s` the threshold. Results are displayed in descending order by their `R` value. In the case of a single input, the surrogate splits do not appear.

```{r summary.single.output, collapse = FALSE}
summary(single_model)
```

`size()` returns the number of leaf nodes of an `EAT` model:
  
```{r size.single.output, collapse = FALSE}
size(single_model)
```

`frontier.levels()` returns the frontier levels of the outputs:
  
```{r frt.single.output, collapse = FALSE}
frontier.levels(single_model)
```

`descrEAT()` returns a list with centralization and dispersion measures and the root mean square error (RMSE) for each leaf node and variable with the following structure:
  
  * `Node`: node index.
  * `n(t)`: number of DMUs at the node.
  * `%`: proportion of DMUs at the node.
  * `mean`: mean.
  * `var`: variance.
  * `sd`: standard deviation.
  * `min`: minimum.
  * `Q1`: first quantile.
  * `median`: median.
  * `Q3`: third quantile.
  * `max`: maximum.
  * `RMSE`: root mean square error at the node.

```{r perf.single.output, collapse = FALSE}
descriptiveEAT <- descrEAT(single_model)

# Descriptive for the nodes 1-3
descriptiveEAT[1:3]
```

Additionally, `EAT_object[["tree"]][[id_node]]` or `EAT_object$tree[[id_node]]` returns a `list` that allows to know in greater detail the characteristics of a given node. The elements that define a node are the following:
  
  * `id` : node index.
  * `F` : father node index. 
  * `SL` : left child node index.
  * `SR` : right child node index.
  * `index` : set of indexes corresponding to the observations in a node.
  * `varInfo` : list containing the error of the left node, the error of the right node and the threshold of the best split for each input.
  * `R` : error at the node.
  * `xi` : index of the variable that produces the split in a node.
  * `s` : threshold of the variable `xi` by which the split takes place.
  * `y` : value(s) of the predicted variable(s) in a node.
  * `a` : lower bound of the given node.
  * `b` : upper bound of the given node.

Note that:

  * The node 1 has a value of -1 in `F` since it has no parent node.
  * A leaf node has a value of -1 in `SL`, `SR`, `s` and `xi` since it is not divided.

```{r node.charac, collapse = FALSE}
single_model[["tree"]][[5]]
```

* Categorical variables

The types of variables accepted by the `EAT` function are the following:
  
```{r table2, echo = FALSE}
types <- data.frame("Variable" = c("Independent variables (inputs)", 
                                   "Dependent variables (outputs)"),
                    "Integer" = c("x", "x"),
                    "Numeric" = c("x", "x"),
                    "Factor" = c("", ""),
                    "Ordered factor" = c("x", ""))

kableExtra::kable(types, align = rep("c", 5)) %>%
  kableExtra::kable_styling("striped", full_width = F)
```

**Factor (not ordered)**
  
```{r continent}
# Transform Continent to Factor
PISAindex_factor_Continent <- PISAindex
PISAindex_factor_Continent$Continent <- as.factor(PISAindex_factor_Continent$Continent)
```

```{r preprocess_factor, error = TRUE, collapse = FALSE}
error_model <- EAT(data = PISAindex_factor_Continent, 
                   x = c(2, 15), # input
                   y = 3) # output
```

**Factor ordered**
  
```{r GDP_PPP_category, collapse = FALSE}
# Cateogirze GDP_PPP into 4 groups: Low, Medium, High, Very High.  
PISAindex_GDP_PPP_cat <- PISAindex
PISAindex_GDP_PPP_cat$GDP_PPP_cat <- cut(PISAindex_GDP_PPP_cat$GDP_PPP,
                                         breaks = c(0, 16.686, 31.419, 47.745, Inf),
                                         include.lowest = T,
                                         labels = c("Low", "Medium", "High", "Very high"))

class(PISAindex_GDP_PPP_cat$GDP_PPP_cat) # "factor" --> error

# It is necessary to indicate order = T, before applying the EAT function

PISAindex_GDP_PPP_cat$GDP_PPP_cat <- factor(PISAindex_GDP_PPP_cat$GDP_PPP_cat, order = T)

class(PISAindex_GDP_PPP_cat$GDP_PPP_cat) # "ordered" "factor" --> correct
```

```{r categorized_model}
categorized_model <- EAT(data = PISAindex_GDP_PPP_cat, 
                         x = c(15, 19), # input
                         y = 3) # output
```

### frontier()

The `frontier` function displays the frontier estimated by the `EAT` function through a plot from `ggplot2`. The frontier estimated by FDH can be plotted too if `FDH = TRUE`. Training DMUs can be showed by a scatterplot if `train.data = TRUE` and its color, shape and size can be modified with `train.color`, `pch` and `size` respectively. Finally, rownames can be included with `rwn = TRUE`.

```{r frontier, eval = FALSE}
frontier(object,
         FDH = FALSE,
         train.data = FALSE,
         train.color = "black",
         pch = 19,
         rwn = FALSE,
         size = 1.5)
```

To continue, the frontier of the previous model is displayed. It can be seen how the frontier obtained by the `EAT` function generalizes the results of the frontier obtained through FDH, thus avoiding overfitting. The boundary estimated through Efficiency Analysis Trees generates 3 steps corresponding to the 3 leaf nodes (nodes 3, 4 and 5) obtained with the `EAT` function. For each of these steps, a frontier level in terms of the output is given with respect to the amount of input used (in this case level of `PFC`). In addition, we can appreciate 6 DMUs in the frontier: ALB (Albania), MDA (Moldova), SRB (Serbia), RUS (Russia), HUN (Hungary) and SGP (Singapur). Note that the first vertical plane of the frontier does not appear, but if it did, ALB would be on it. These DMUs are efficient and the rest of the DMUs below its specific step should increase the amount of output obtained or reduce the amount of input utilized until reaching the boundary to be efficient. 

```{r single.output.frontier, fig.width = 7.2, fig.height = 6}
frontier <- frontier(object = single_model,
                     FDH = TRUE, 
                     train.data = TRUE,
                     rwn = TRUE)

plot(frontier)
```

* Is the number of steps in the frontier always the number of leaf nodes?
  
  The answer is no. Note that there may be situations where the estimation of two or more nodes is identical. This is necessary to ensure the estimation of an increasing monotonic frontier. In this case, the number of leaf nodes is 5, however the prediction for nodes 4 and 5 is the same and therefore the border only has 4 steps.

```{r single.output.max.depth, collapse = FALSE}
single_model_md <- EAT(data = PISAindex, 
                       x = 15, # input 
                       y = 3, # output
                       max.depth = 5) 
```

```{r size.single.output_md, collapse = FALSE}
size(single_model_md)
```

```{r pred.single.output_md, collapse = FALSE}
single_model_md[["model"]][["y"]]
```

```{r single.output.frontier_md, fig.width = 7.2, fig.height = 6}
frontier_md <- frontier(object = single_model_md,
                        train.data = TRUE)

plot(frontier_md)
```

## Modeling a multioutput scenario. Feature selection.

* Example 2: `S_PISA + R_PISA + M_PISA ~ NBMC + WS + S + PS + ABK + AIC + HW + EO + PR + PFC + I + AAE + GDP_PPP`

```{r multioutput.scenario, collapse = FALSE}
multioutput_model <- EAT(data = PISAindex, 
                         x = 6:18, # input 
                         y = 3:5) # output
```

### rankingEAT()

The second example presents a multiple output scenario where 13 inputs are used to model the 3 available outputs. In these situations, a selection of the most contributing variables may be recommended in order to reduce overfitting, improve precision and reduce future training times. `rankingEAT()` allows a selection of variables by calculating a score of importance through Efficiency Analysis Trees technique. The user can specify the number of decimal units (`r`), include a barplot with the scores of importance (`barplot`) and display a horizontal line in the graph to facilitate the cut-off point between important and not relevant variables (`threshold`).

```{r ranking, eval = FALSE}
rankingEAT(object,
           r = 2,
           barplot = TRUE,
           threshold = 70)
```

The importance score represents how influential the variable is in the model. In this case, the cut-off point is set at 70 and therefore important variables are considered: **AAE** (Acess to Advance Education), **WS** (Water and Sanitation), **NBMC** (Nutrition and Basic Medical Care), **HW** (Health and Wellness) and **S** (Shelter). 

```{r multioutput.importance, fig.width = 7.2, fig.height = 6}
rankingEAT(object = multioutput_model,
           r = 2,
           barplot = TRUE,
           threshold = 70)
```

## Graphical representation by a tree structure

### plotEAT()

`frontier()` allows us to see the regions of the input space originated with `EAT()` in a very clear way, however, this is impossible with more than two variables (one input and one output). For multiple input and / or output scenarios, it is provided the typical tree-structure in which the relations between the predicted variable(s) and the predictive variable(s) are showed.

In each node, we can obtain the following information:
  
  * `id`: node index.
  * `R`: error at the node.
  * `n(t)`: number of DMUs at the node.
  * `input name` by which the split take place.
  * `y` : vector of predictions.

Furthermore, the nodes are colored according to the variable by which the division is performed or they are black in the case of being a leaf node.

```{r plotEAT, eval = FALSE}
plotEAT(object)
```

* Example 3: `S_PISA + R_PISA + M_PISA ~ NBMC + WS + S + HW + AAE`

```{r model.graph1, collapse = FALSE}
best_multioutput <- EAT(data = PISAindex, 
                        x = c(6, 7, 8, 12, 17), # input
                        y = 3:5, # output
                        numStop = 8,
                        fold = 6)
```

```{r graph1, fig.dim = c(8.4, 7.5)}
plotEAT(object = best_multioutput)
```

* Example 4: `S_PISA ~ NBMC + S + HW + I + GDP_PPP`

```{r model.graph2, collapse = FALSE}
best_multioutput2 <- EAT(data = PISAindex, 
                         x = c(6, 8, 12, 16, 18), # input
                         y = 3) # output
```

```{r graph2, fig.dim = c(8.4, 7.5)}
plotEAT(object = best_multioutput2)
```

## EAT tuning

In this section, `PISAindex` is divided into a training subset with 70% of the DMUs and a test subset with the other 30% of the DMUs. Next, the `bestEAT` function is applied to find the value of the hyperparameters `numStop` and `fold` that minimize the error calculated on the test sample from a Efficiency Analysis Tree trained with the training sample.

```{r training_test}
n <- nrow(PISAindex) # Observations in the dataset
t_index <- sample(1:n, n * 0.7) # Training indexes
training <- PISAindex[t_index, ] # Training set
test <- PISAindex[-t_index, ] # Test set
```

### bestEAT()

The `bestEAT` function requires a training set (`training`) on which to model an Efficiency Analysis Tree and a test set (`test`) on which to calculate the error. The number of trees built is given by the number of different combinations that can be given by the `numStop`, `fold` and `max.depth` arguments. `bestEAT()` returns a data frame with the following columns:
  
  * `numStop` : numStop hyperparameter value.
  * `fold` : fold hyperparameter value.
  * `max.depth` : max.depth hyperparameter value if it does not set to `NULL`.
  * `RMSE`: root mean square error calculated on the test sample with the tree built with the training sample, numStop, fold and max.depth (if not `NULL`) values.
  * `leaf`: number of leaf nodes of the tree.

```{r bestEAT, eval = FALSE}
bestEAT(training, test,
        x, y,
        numStop = 5,
        fold = 5,
        max.depth = NULL,
        na.rm)
```

Tuning for: 
  
`S_PISA + R_PISA + M_PISA ~ NBMC + WS + S + PS + ABK + AIC + HW + EQ + PR + PFC + I + AAE + GDP_PPP`

`numStop = {3, 5, 7, 10}` and `fold = {5, 7}`

```{r eat.tuning1, collapse = FALSE}
bestEAT(training = training, 
        test = test,
        x = 6:18,
        y = 3:5,
        numStop = c(3, 5, 7, 10),
        fold = c(5, 7))
```

Tuning for: 
  
`S_PISA + R_PISA + M_PISA ~ NBMC + WS + S + HW + AAE`.

`numStop = {3, 5, 7, 10}` and `fold = {5, 7}`

```{r eat.tuning2, collapse = FALSE}
bestEAT(training = training, 
        test = test,
        x = c(6, 7, 8, 12, 17),
        y = 3:5,
        numStop = c(3, 5, 7, 10),
        fold = c(5, 7))
```

The best Efficiency Analysis Trees is given by the hyperparameters `{numStop = 3, fold = 7}` in the reduced model with `RMSE = 46.17` and 24 leaf nodes. However, this model is too complex. Therefore, we select the model with parameters `{numStop = 7, fold = 5}` with `RMSE = 51.88` but with only 12 leaf nodes.

```{r bestmodel, collapse = FALSE}
bestmodel <- EAT(data = PISAindex,
                 x = c(6, 7, 8, 12, 17),
                 y = 3:5,
                 numStop = 7,
                 fold = 5)
```

```{r summary.bestmodel, collapse = FALSE}
summary(bestmodel)
```

## Efficiency scores. Graphical representation.

### Efficiency Analysis Trees model: efficiencyEAT()

The efficiency scores are numerical values that indicate the degree of efficiency of a set of DMUs. It must be entered a dataset (`data`) and the corresponding indexes of input(s) (`x`) and output(s) (`y`). It is recommended that the dataset with the DMUs whose efficiency is to be calculated coincide with those used to estimate the frontier. However, it is also possible to calculate the efficiency scores for a new dataset. The efficiency scores are calculated using the mathematical programming model included in the argument `score_model`. The following models are available:
  
  * `BCC_out`: Banker Charnes and Cooper output-oriented radial model with efficiency level at 1.
  * `BCC_in`: Banker Charnes and Cooper input-oriented radial model with efficiency level at 1.
  * `DDF`: Directional Distance Function with efficiency level at 0.
  * `RSL_out`: output-oriented Rusell Model with efficiency level at 1.
  * `RSL_in`: input-oriented Rusell Model with efficiency level at 1.
  * `WAM`: Weighted Additive Model with efficiency level at 0.

If `FDH = TRUE`, scores are also calculated through a FDH model. Finally, a brief summary of the distribution of the scores calculated for each model is also included.

```{r efficiencyEAT, eval = FALSE}
efficiencyEAT(data, x, y, 
              object,
              score_model,
              r = 2,
              FDH = TRUE,
              na.rm = TRUE)
```

For this section, the `single_model` previously created is used.

```{r scoresEAT, collapse = FALSE}
scores_EAT <- efficiencyEAT(data = PISAindex,
                            x = 15, 
                            y = 3,
                            object = single_model, 
                            scores_model = "BCC_out",
                            r = 3,
                            FDH = TRUE)
```

```{r scoresEAT2, collapse = FALSE}
scores_EAT2 <- efficiencyEAT(data = PISAindex,
                             x = 15, 
                             y = 3,
                             object = single_model, 
                             scores_model = "BCC_in",
                             r = 3)
```

### Convex Efficiency Analysis Trees model: efficiencyCEAT()

`efficiencyCEAT()` returns the efficiency scores for the convex frontier obtained through Efficiency Analysis Trees. In this case, If `DEA = TRUE` scores are also calculated through a DEA model.

```{r efficiencyCEAT, eval = FALSE}
efficiencyCEAT(data, x, y, 
               object,
               score_model,
               r = 3,
               DEA = TRUE,
               na.rm = TRUE)
```

```{r scoresCEAT, collapse = FALSE}
scores_CEAT <- efficiencyCEAT(data = PISAindex,
                              x = 15, 
                              y = 3,
                              object = single_model, 
                              scores_model = "BCC_out",
                              r = 3,
                              DEA = TRUE)
```

### efficiencyJitter()

`efficiencyJitter` returns a jitter plot from `ggplot2`. This graphic shows how DMUs are grouped into leaf nodes in a model built using the `EAT` function. Each leaf node groups DMUs with the same level of resources. The dot and the black line represent, respectively, the mean value and the standard deviation of the scores of its node. Additionally, efficient DMU labels always are displayed based on the model entered in the `score_model` argument. Finally, the user can specify an upper bound (`upb`) and a lower bound (`lwb`) in order to show, in addition, the labels which efficiency score is between them. Scores from a convex Efficiency Analysis Tree (`CEAT`) model can also be used.

```{r efficiency_jitter, eval = FALSE}
efficiencyJitter(object, scores_EAT,
                 scores_model,
                 lwb = NULL, upb = NULL)
```

```{r jitter_single, collapse = FALSE, fig.width = 7.2, fig.height = 5}
efficiencyJitter(object = single_model,
                 scores_EAT = scores_EAT$EAT_BCC_out,
                 scores_model = "BCC_out",
                 lwb = 1.2)
```

```{r jitter_single2, collapse = FALSE, fig.width = 7.2, fig.height = 5}
efficiencyJitter(object = single_model,
                 scores_EAT = scores_EAT2$EAT_BCC_in,
                 scores_model = "BCC_in",
                 upb = 0.65)
```

Graphically, for a single input and output scenario it is observed that if the BCC models are used to obtain the efficiency scores:
  
  * Under output orientation, those DMUs that are arranged in the horizontal plane of the frontier are efficient.

  * Under input orientation those DMUs that are arranged in the vertical plane of the frontier are efficient.

  * If a DMU is located in a corner of the frontier, it is efficient under both orientations.

```{r frontier_comparar, fig.width = 7.2, fig.height = 6, fig.align = 'center'}
plot(frontier)
```

### efficiencyDensity()

`efficiencyDensity()` returns a density plot from `ggplot2`. In this way, the similarity between the scores obtained by the different available methodologies can be verified. 

```{r efficiency_density, eval = FALSE}
efficiencyDensity(scores,
                  model = c("EAT", "FDH"))

```

```{r density_single, collapse = FALSE, fig.width = 7.2, fig.height = 6, fig.align = 'center'}
efficiencyDensity(scores = scores_EAT[, 3:4],
                  model = c("EAT", "FDH"))

efficiencyDensity(scores = scores_CEAT[, 3:4],
                  model = c("CEAT", "DEA"))

```

* The curse of dimensionality

When the ratio of the sample size and the number of variables (inputs and outputs) is low, the standard methods of efficiency analysis (specially FDH) tend to evaluate a large number of DMUs as technically efficient. This problem is known as **curse of dimensionality**. To show it, the efficiency scores of the `multioutput_model` (section 2) with 16 variables and 71 DMUs are calculated:
  
```{r cursed.scores, collapse = FALSE}
cursed_scores <- efficiencyEAT(data = PISAindex,
                               x = 6:18, 
                               y = 3:5,
                               object = multioutput_model, 
                               scores_model = "BCC_out",
                               r = 3,
                               FDH = TRUE)
```

```{r cursed.density, collapse = FALSE, fig.width = 7.2, fig.height = 6, fig.align = 'center'}
efficiencyDensity(scores = cursed_scores[, 17:18],
                  model = c("EAT", "FDH"))

```

## Random Forest

### RFEAT()

Random Forest + Efficiency Analysis Trees (`RFEAT`) has also been developed with the aim of providing a greater stability to the results obtained by the `EAT` function. The `RFEAT` function requires the `data` containing the variables for the analysis, `x` and `y` corresponding to the inputs and outputs indexes respectively, the minimun number of observation in a node for a split to be attempted (`numStop`) and `na.rm` to ignore observations with `NA` cells. All these arguments are used for the construction of the `m` individual Efficiency Analysis Trees that make up the random forest. Finally, the argument `s_mtry` indicates the number of inputs that can be randomly selected in each split. It can be set as any integer although there are also certain predefined values. Being, $n_{x}$ the number of inputs, $n_{y}$ the number of outputs and $n(t)$ the number of observations in a node, the available options in `s_mtry` are: 
  
  * `Breiman` = $\frac{n_{x}}{3}$
  * `DEA1` = $\frac{n(t)}{2} - n_{y}$
  * `DEA2` = $\frac{n(t)}{3} - n_{y}$
  * `DEA3` = $n(t) - 2 \cdot n_{y}$
  * `DEA4` = $min(\frac{n(t)}{n_{y}}, \frac{n(t)}{3} - n_{y})$
  
The function returns a `RFEAT` object.

```{r RF, eval = FALSE}
RFEAT(data, x, y,
      numStop = 5, m = 50,
      s_mtry = "Breiman",
      na.rm = TRUE)
```

```{r RFmodel}
forest <- RFEAT(data = PISAindex, 
                x = 6:18, # input 
                y = 5, # output
                numStop = 5, 
                m = 40,
                s_mtry = "Breiman",
                na.rm = TRUE)
```

```{r print.RFEAT, collapse = FALSE}
print(forest)
```

## plotRFEAT()

`plotRFEAT()` returns the Out-Of-Bag error for the training dataset and a forest consisting of k trees.

```{r plot.RFEAT, collapse = FALSE, fig.width = 7.2, fig.height = 6}
plotRFEAT(forest)
```

Note that the OOB error of early forests suffers from great variability.

## rankingRFEAT()

As in `rankingEAT()`, the `rankingRFEAT` function allows to calculate an importance score for variables using an `RFEAT` object.

```{r rankingRFEAT, eval = FALSE}
rankingRFEAT(object, r = 2,
             barplot = TRUE)
```

```{r rankingRFEAT_forest, fig.width = 7.2, fig.height = 6}
rankingRFEAT(object = forest, r = 2,
             barplot = TRUE)
```

* A positive importance score means that including the input in the model improves the performance.

* A negative imporance score means that removing the input from the model improves the performance.

Note the peculiarity of the results shown where no variable has a positive importance score.

## bestRFEAT()

As in `bestEAT()`, the `bestRFEAT` function is applied to find the optimal hyperparameters that minimize the root mean square error (RMSE) calculated on the test sample. In this case, the available hyperparameters are `numStop`, `m` and `s_mtry`. Note that `s_mtry` is a character vector even if integer values are introduced.

```{r bestRFEAT, eval = FALSE}
bestRFEAT(training, test,
          x, y,
          numStop,
          m,
          s_mtry,
          na.rm = TRUE)
```

```{r tuning.bestRFEAT, collapse = FALSE}
bestRFEAT(training = training, 
          test = test,
          x = 6:18,
          y = 3:5,
          numStop = c(3, 5, 10),
          m = c(30, 40, 50),
          s_mtry = c("Breiman", "3"))
```

The best Random Forest + Efficiency Analysis Trees model is given by the hyperparameters `{numStop = 3, m = 30, s_mtry = "Breiman"}` with `RMSE = 44.60`.

## efficiencyRFEAT()

As in `efficiencyEAT()`, the `efficiencyRFEAT` function returns the efficiency scores for a set of DMUs. However, in this case it is only available for the BCC model with output orientation. Again, the FDH scores can be requested using `FDH = TRUE`.

```{r eff_scores, eval = FALSE}
efficiencyRFEAT(data, x, y,
                object,
                r = 2,
                FDH = TRUE)
```

```{r scores_RF}
scoresRF <- efficiencyRFEAT(data = PISAindex,
                            x = 6:18, # input
                            y = 5, # output
                            object = forest,
                            FDH = TRUE)
```

## Predictions

`predictEAT()` and `predictRFEAT()` return a data frame with the data and the expected output for a set of observations using Efficiency Analysis Trees and Random Forest + Efficiency Analysis Trees techniques respectively. In both cases, `newdata` refers to a data frame with the input variables. Regarding the `object` argument, in the first case it corresponds to an `EAT` object and in the second case to a `RFEAT` object.

In predictions using an `EAT` object, only one Efficiency Analysis Tree is used. However, for the `RFEAT` model, the output is predicted by each of the `m` individual trees trained and subsequently the mean value of all predictions is obtained.

### predictEAT()

```{r predictEAT, eval = FALSE}
predictEAT(object, newdata)
```

### predictRFEAT()

```{r predictRFEAT, eval = FALSE}
predictRFEAT(object, newdata)
```

### predictFDH()

`predictFDH()` returns a data frame with the data and the expected output for a set of observations using the Free Disposal Hull mathematical programming model. In this case, it is necessary to enter the data set (`data`) with the study variables, the indexes of the inputs (`x`) and the indixes of the outputs (`y`).

```{r predictFDH, eval = FALSE}
predictFDH(data, x, y)
```

```{r models, collapse = FALSE}
input <- c(6, 7, 8, 12, 17)
output <- 3:5

which(is.na(PISAindex), arr.ind = TRUE)

# FDH does not accept NA rows, so we exclude ESP

EAT_model <- EAT(data = PISAindex[- 32, ],
                 x = input,
                 y = output)

RFEAT_model <- RFEAT(data = PISAindex[- 32, ],
                     x = input,
                     y = output)
```

```{r predictions, collapse = FALSE}
predictions_EAT <- predictEAT(object = EAT_model,
                              newdata = PISAindex[- 32, input])

predictions_RFEAT <- predictRFEAT(object = RFEAT_model,
                                  newdata = PISAindex[- 32, input])

predictions_FDH <- predictFDH(data = PISAindex[- 32, ],
                              x = input,
                              y = output)
```

```{r EAT_vs_RFEAT_vs_FDH, collapse = FALSE}
predictions <- cbind(PISAindex[- 32, 3], PISAindex[- 32, 4], PISAindex[- 32, 5], 
                     predictions_EAT[, 6], predictions_EAT[, 7], predictions_EAT[, 8],
                     predictions_RFEAT[, 6], predictions_RFEAT[, 7], predictions_RFEAT[, 8],
                     predictions_FDH[, 6], predictions_FDH[, 7], predictions_FDH[, 8]) %>%
  as.data.frame()

names(predictions) = c("S_PISA", "R_PISA", "M_PISA",
                       "S_EAT", "R_EAT", "M_EAT",
                       "S_RFEAT", "R_RFEAT", "M_RFEAT",
                       "S_FDH", "R_FDH", "M_FDH")

kableExtra::kable(predictions) %>%
  kableExtra::kable_styling("striped", full_width = F) %>%
  kableExtra::column_spec(c(1, 2, 3), background = "#DBFFD6") %>%
  kableExtra::column_spec(c(4, 5, 6), background = "#FFFFD1") %>%
  kableExtra::column_spec(c(7, 8, 9), background = "#FFCCF9") %>%
  kableExtra::column_spec(c(10, 11, 12), background = "#F4F1BB")

```
